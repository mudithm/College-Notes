<h1>Lecture 3</h1>

---

  * Ultimately, even if a function matches all the training data, it might not be correct; it must also perform well on random data

<h5>Hypothesis space</h5>

  * Linear model `w_T x + b = 0`
      - w and b are found through learning, and the goal is to find values for both that best fit the data
  * We can instead approximate the risk by the training error/ empirical risk
  * Reasonable when:
      - both the training data and test set are based on this distribution

---

<h5>overfitting the data</h5>

  * Getting your model to perfectly match the training data might not be the best thing
      - it might overfit and find identify patterns that do not exist in reality, and result in bad predictions


<h5>Bias vs Variance</h5>

  * Training data are subsamples of the true distribution
  * Exam strategy:
      - study every chapter well
          + A+ Low variance and Bias
      - study only a few chapters well
          + A+ ? B ? C ? Low Bias, high variance
          + might have a good chance if you get lucky
      - Study every chapter roughly
          + B+ High bias, low variance
          + about the same understanding of every chapter
      - Go to sleep 
          + B ~ D: High variance, high bias
          + no understanding of any chapter 
  * In most cases, underfitting is easy to identify, but overfitting is more difficult to observe

<h5>Preventing overfitting</h5>

  * Use a less-expressive model
      - EG linear model
  * Add regularization
      - promote simper methods
  * Data perturbation
      - make the model more roust
      - can be done algorigthmically through dropout
  * Stop the optimization earlier

---

<h5>An example of regression</h5>

  * Consider simple regression dataset
      - `f:X->Y`
      - `x in R`
      - `y in R` 
  * Question 1: How to pick hypo space?
      - Degree-M Polynomials
      - infinitely many hypotheses
      - Which is best?
      - Common choice to find error is squared loss
          + `(3.1)`
      - Empirical loss of functin h applied to training data is then 
          + `(3.2)`
      - How to find the best M?
      - M is a hyper-parameter for reg model
      - we can try out diff ms and see which one works
      - How?
      - __Train/Dev/Test splits__
          + You need to report performance on test data, but you aren't allowed to look at it (or else it acts like training data)
          + Split your training into two sets
              * Training Data (~80-90%)
              * Development Data (~10-20%)
              * You are allowed to look at the dev data and use it to tweak parameters
              * `(3.3)`

<h5>Recipe of Train/Dev/Test</h5>

  * For each poss value fo the hyper param e.g M = 1, 2, 3, 4...10
      - Train a model uing D_train
      - eval using D_dev
  * Choose model param with best perf on D_dev
  * (optional) Retrain on D_train U D_dev with the best param set
  * Evaluate the model on D_test

<h5>Tradeoff bt Train vs Dev Size</h5>

  * Consider 100  pts
      - Train: 95 Dev: 5
          + too small dataset for evaluating model; if too noisy, bad results
      - Train: 50 Dev: 50
          + 50 pts is prolly too small to train the model

---

<h5>N-fold crosss validaiton</h5>

  * Instead of a single test-training split, split into N equal sized parts, train and test N different classifiers
      - use a different section of hte data for dev for each classifier
  * report avg acc and standard dev of the accuracy

---

<h5>K-nearest neighbor</h5>

  * See similar adjacent stuff to make predictions/decisions
  * group similar things in regions based on some features, and see the distance between the test point and the cluster in the space
      - smallest distance determines the classification
  * Nearest-Neighbor: basic version
      - decide based on closest one
  * k-nearest neighbors:
      - decide based on k closest ones

<h5>Issues in KNN algorithm</h5>

  * How to find distance?
      - Euclidean distance
      - Manhattan Distance
          + add x difference, y difference, etc (orthogonal walking)
      - Lp-Norm
      - Non-numerical:
          + symbolic/categorical features
          + Hamming distance
              * how many bits are different or number of features that have a different value
              * EX:
                  - {shape=`triangle`, color=red, location=left, orientation=`up`}
                  - {shape=`circle`, color=red, location=left, orientation=`down`}
                  - distance is 2 (two features are different)
  * How to choose k?
      - Hyperparameter tuning
          + Can use train/dev/test or n-fold cross validation
      - In practice: choose an odd number, so you don't get a tie
      - Additionally, you can weight the labels by their distance (related to Kernel method)
      - Normalize your feature
          + different features could have different scales (height, weight, etc) but distance would treat them equally
          + Preprocess data to have xero mean and unit standard dev in each dimension
          + scale the feature accordingly (peruse the slides, slide #73)
          + You can include the test data for these stuff instead, if it is available

<h5>Other hyperparameters for KNN</h5>

  * choosing K
  * Distance measurement mode (lp, l1, etc)

---