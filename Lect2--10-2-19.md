<h1>Lecture 2</h1>

---

<h5>What is Learning?</h5>

  * The Badges Game
      - Example of Supervised Learning
      - Put a plus or minus sign in front of nametag at the 1994 machine learning conference
      - Goal was to have people guess what the rule was
          + it was that: if the second letter of your name is a vowel, you get a plus, otherwise a minus
  * Labeled test data vs raw data: need to make sure the model is not merely remembering the test data, but that it is actually making some kind of generatlization

---

<h5>Supervised Learning</h5>

  * Systems apply a function f() to input items x and return an output y = f(x)
  * Input set X, output set Y
  * Build a model g(x) to approximate the target function f(x)
  * Training:
      - Labeled training data is supplied to the learner, and it decides which parameters are most important. The learner returns a model based on its learning
  * Testing
      - apply the model to the raw test data
      - see if the model-predicted labels match the test labels

<h5>Learning the Mapping</h5>

  * What are the key questions when desiging a learning system?
      - Modeling
          + how to formulate problems as ML problems
          + learning protocols - where are the data/labels coming from?
      - Representation
          + What functions hshuod we learn (hypothesis spaces)
          + how to map raw input to an instance space
      - Algorithms
          + What are good algs?
          + how do we define success?
          + the computational problem

<h5>Using Supervised Learning</h5>

  * What is our input space
      - what kind of fearuees are we using
  * What is our label space
      - what kind of learning task are we daling with
  * What is our hypothesis sace
      - what kind of functions are we learning
  * what learning algorithm do we use
      - how do we learn the model from the laeled data
  * What is our loss evaluation function?

---

<h5>Input: the instance space X</h5>

  * x is represented in a feature space
      - typically x is in (0, 1)^n or R^n
      - usually repd as a vector
      - called input vector
  * Example: 
      - Boolean features
          + does this email contain the word "money?"
      - Numerical features: 
          + How often does the word "money" occur in the email
          + etc
      - Other Features
          + related to sender's address, etc

<h5>X as a vector space</h5>

  * X is an N-dimensional vector space (e.g. R^N)
      - each dimension is one feature
  * each __x__ in X is a feature vector
      - Think of __x__ = [x1, x1, ..., xn] 

<h5>From feature templats to vectors</h5>

  * When desinging feaurs, think of templates, not ind features
  * encoding name-by-name by encoding it in characters
      - What is the ith letter?
          + "abe" -> [1000...01000..00001...]
          +  ...................__a__bcd...a __b__ cd...abcd __e__
          + 26*2 + 1 positions in each group (upper, lower, space);
          + # of groups = upper bounds on the length of names

<h5>Good features are essential</h5>

  * The choice of features is crucial for how well a task can be learned
      - in many application areas, (language, vision, etc), a lot of work goes into desigini suitable features
      - this requires domain expertise

---

<h5>Output Space</h5> 

  * y is represented in output/label space
  * diff kinds of output
      - binary classification {-1, 1}
      - multicalls classification {1, 2, 3, ... K}
      - regression {R}
      - structured output {1, 2, 3, ... K}^N

---

<h5>Supervised Learning: examples</h5>

  * Disease DIagnosis
      - input: properties of pationt (symptoms, lab tests)
      - output: disease, or recommended therapy
  * Part-of-Speech tagging
      - input: an english sentence
      - output: the part of speech of a word in the sentence
  * Face recog
      - input: bitmap picture of person's face
      - output: name or property of the person

<h5>The model g(x)</h5>

  * We need to choose what kind of model we want to learn
  * A four-bit input signal has 16 possibilities; there are 2^16 possible binary functions on these input possibilities, so if we don't make any assumptions, that is 65536 possibilites to go through
  * But, if we are given the answers to 7 of the possibilities, we only have 9 left to guess; therefore, we have 2^9 possibilities remaining.
  * There are Y^X possible functions f(x) from the instance space X to the label space Y, if no assumptions are made
  * Simple Rules: there are only 16 simple conjunctive rules of the form y = xi conj xj conj xk
  * ie, if the rule is x1, then if x1 is true the output is ture
  * or if the rule is x1 and x1, then if both are true the output is treu
      - etc until test all combos
      - in the case of the example, none of the simple conjunctive rules apply
      - They hypothesis space might be too narrow, so we can't find a function that fits the data. Otherwise, there might be noise in the inputs
  * Hypothesis space 3
      - m-of-n rules: there are 32 possibe rules of the form "y = 1 iff at least m of the following n variables are 1"
  * We can continue to do similar eliminations until we find a consistent hypothesis

<h5>Views of Learning</h5>

  * Learning is the removal of our remaining uncertainty
      - We could use training data to infer which func it is
  * Learning requires guessing a good, small hypothesis class
      - can start with a small calss and enlarge it until it contains a hypothesis that fit the data
  * We can always be wrong at this stage!

<h5>General Strategies for Machine Learning</h5>

  * Develop flexible hypothesis spaces
      - decision trees, neural networks, nested colecctions might not need as narrow a hypothesis, but require more data
  * Develop representation languages for restricted class functions
      - serve to limit the expressivity of the target models
          + eg, functional representation, Grammars, linear functions, stochastic models
      - Get flexibility by augmenting the feature space
  * In either case:
      - develop algorithms for finding a hypothesis in our hypothesis space, that fits the data
      - hope they will generalize well lol

<h5>Real world example</h5>

  * "i don't know {whether, weather} to laugh or cry"
      - how can we make this a learning problem?
      - We look for a functin (sentence -> whether/weather)
      - we define the domain better
          + fo each word w in ENglish define a boolean feature x_w
              * x_w = 1 iff w is in the sentence
          + This maps a sentence to a point in {0, 1}^50,000
      - In this space: some points are whether points, some are weather points
  * Hypothesis space: 
      - boolean feature x_w, x_w == 1 iff w in sentence 
  * What learning model should we apply?
  * Learning problem:
      - find functions that best separate the data
      - A possibility: define the learning problem to be a linear function that best separates the data
          + Linear = linear in the feature space
          + x = data representation, w = the classifier
          + y = sgn{wTx}

<h5>Expressivity: Linear Classifier</h5>

  * also written as <x, w> or xTw
  * f(x) = sgn{x dot w - theta} = sgn{sum(w_i x_i) - theta}
  * Many functions are linear
      - conjunctions:  
          + y = x1 and x3 and x5
          + approx equiv to: y = sgn(x1 + x3 + x5 - 3)
      - at least m of n:
          + y = at least 2 of {x1, x3, x5}
          + approx equiv to: y = sgn(x1 + x3 + x5 - 1)
  * many others are not
      - xor, etc
  * Functions can be made linear
      - data are not linearly separable in one dimension if you insist on only using one-dimensional functions
      - say we have a linear space based on x (like on a number line): change into <x. x^2> (no new info, just a new function)

<h5>How to learn?</h5>

  * How can we find a good model from the hypothesis space?
  * Challenges: the hyp space contains an inf # of functions
  * sev functions can be consistent with the data
  * A possibility: local search
      - start with linear threshold
      - see how your doing
      - make changes as necessary
  * Search can be done in a smarter way
      - do not direectoy search in the hypothesis space

<h5>General framework for learning</h5>

  * Problem Setting
      - set of possible instances X
      - set of poss labels Y
      - Unknown target func f:X->Y
      - Set of function hypotheses H={h|h:H->Y}
  * Input: training instances drawn from data generating distribution rho
  * Output: hypothesis h in H tha best approximates f
      - Formally, h should have low expected (test) loss/Risk
      - Expected value(x, y)~p[L(y, h(x))] = sum(p(x,y)L(y, h(x)))
  * Problem?    
      - We don't know what rho is
      - but we are given samples drawn from rho
  * We instead approximate the risk by the training error/empirical risk
      - 1/n * sum from 1 to n(:(y, h(x_i)))
  * 
