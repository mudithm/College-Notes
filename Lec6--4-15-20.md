<h1>Lecture 6</h1>

---

<h3>Recap</h3>

  * Part 1 of lecture notes
      - not included for the midterm -- number theory
  * Part 2 -- only up to 3/4 + &epsilon; probability proof
      - hardcore bits
  * Part 3 (today and into next week)
      - PRG, PRF
  * Before midterm:
      - parts 4, 16, 5 -- Then midterm!

---

<h3>Computational indistinguishability</h3>

  * __F6.1__
  * D1 and D2 are computationally indistinguishable if no poly-time judge can tell them apart
  * Sampleable distribution
      - distribution efficiently generated by a poly-time machine
      - Ex: uniform distribution
      - PRG that takes a small seed and outputs a output
  * Judge
      - Machine in PPT that outputs 0/1
      - __F6.2__
      - X<sub>n</sub> and Y<sub>n</sub> are poly-time indistinguishable if &forall; c &forall; J &isin; PPT, &exist; N  &forall; n &gt; N
          + | `Pr[`J(x &larr; X<sub>n</sub>)=1`]` - `Pr[`J(x &larr; Y<sub>n</sub>)=1`]`  | &lt; 1/n<sup>c</sup>
          + In plain English, the probability that the judge predicts that the distribution is from X<sub>n</sub> and the probability that the distribution is from Y<sub>n</sub> is basically the same for large n
          + X<sub>n</sub> and Y<sub>n</sub> are distributions of n bits/digits
              * This is an asymptotic definition; for small n, there is some tiny prob that the judge can distinguish, but as n gets larger this becomes negligible.
      - Similar to Turing Test for Intelligence
          + Human is the judge
          + but, given many samples, not just one
          + however,
              * THM: if &exist; a PPT judge J that can distinguish X<sub>n</sub> from Y<sub>n</sub> on poly-many samples (of X<sub>n</sub> or Y<sub>n</sub>) &rArr; &exist; PPT J' that can distinguish on a single sample.
                  - So, giving many examples does not make the judges any more power than a single question
      - Pf of theorem:  
          + __F6.3__
          + Using J, one can construct J' such that J' can distinguish X<sub>i</sub> and Y<sub>i</sub> with probability &epsilon;/n
          + Hybrid distributions (replace one sample from X with one from Y, etc)
          + Claim 1: &exist; two adjacent distributions (P<sub>k</sub>, P<sub>k+1</sub>)S such that J on input these two adjacent distributions distinguishes the two with probability greater than &epsilon;/n\
          + Note: P<sub>i</sub> is the probability that the judge will output 1 on that distribution
          + To create J', we take these two adjacent distrs and insert a single mystery sample at the point the two adjacent distrs differ, and passes the new distr to J.
              * can get the adj distrs by being given them, or just guess w prob 1/n, or test on a bunch of samples, etc.
          + This way, we have a greater prob of guessing but only use one sample.

---

<h3>Pseudo-Random Generators</h3>

  * __F6.4__
  * DEF: PRG is secure if its output is indistinguishable from uniform distribution of the same length
  * DEF2: PRG is secure if it passes "next bit test"
  * Next bit test: 
      - Challenger uses a random seed to generate a string of length n^3 (or some poly)
      - Challenger sends a bit, and adversary sends "0" or "1"
          + 0 means stop, 1 means continue
      - As long as the ch receives 1, it continues to send bits in sequence
      - When the adversary sends the stop, it also sends its prediction for the next bit
      - Adversary wins if this prediction is correct with prob > 1/2 + 1/n<sup>c</sup>
  * Def: PRG is secure (for next bit test) if &nexist; an adversary ADV &isin; PPT that can win the next bit test.
  * Next bit test security &rArr; indistinguishability security 
      - Proven with contrapositive
      - __F6.5__
      - Similar hybrid argument than the one we did before.