<h1>Lecture 5</h1>

---

<h4>Decision Boundary</h4>

<h5>Decision Tree Recap</h5>

  * Data processed in batch (all the data available)
  * Recursively biold dec tree top down
  * Learns by splitting the input space

<h5>KNN Recap</h5>

  * Learning by memorization
  * Decision boundary for KNN:
      - Is the K Nearest Neighbors alg explicityly building a function?
          + No, it never forms an _explicit_ hypothesis
      - Given a training set, what is the _implicit_ function that is being computed?
      - Example: Voronoi Diagram
          + Voronoi cell of x i a polytope consisting of all points closer to x than to any other points in S
          + It covers the entire space
          + Can be used to identify decision boundaries for 1-NN model

<h5>Instance Based Learning</h5>

  * Class of learning methods
      - Learning: store examples w labels, index
      - Prediction: when presented a new ex, classify labels using similar stored exs
  * K-nearest is an instance-baed alg
  * Most of computation is performed only at prediction time
      - open book vs closed book exams -- this is more like an open book exam; do not prep earlier

<h5>KNN vs Decision Tree</h5>

  * KNN training is v fast
      - adding instances to a list, perhaps some indexing
  * Can learn very complex functions
  * always have training data
  * Hovevre, KNN needs a lot of storage, DT is more cmpact
  * Prediction can be v slow
      - naively: O(dN) for N training exs in d dimensions
      - More data makes it slower
  * Nearest neighbrs are fooled by irrelevant attrs
      - important and subtle
  * Curse of dimensionality
      - things can go wron in the hing-rimensional spaces
      - what migth be intuitive for 2-3 dims does not always apply to high dim spaces
      - enclosed boundaries that take up most of the space in 2d can take close to none in 3d+.
          + ie inscribed sphere in cube:
              * 2D: ~80% chance it's in circle
              * 3D: ~50% chance it's in circle
              * ND: as N->inf, ~0% chance it's in circle
          + THe opposite can apply to other geometries
              * ex: two co-centric spheres with similar radii
                  - small D, most space inside inner sphere
                  - large D, most space bt spheres
  * Essentially, the "Neighborhood" can become very large



<h5>Dealing with the curse of dimensionality</h5>

  * Most "real-world" data is not uniformly dist in high-dimensional space
      - dimensionality reduction (ie projection onto lower dim, etc)
  * Feature selection
      - decide which features give most info (ie by information gain)
  * Prior knowledge often helps


---


<h4>Perceptron</h4>

  * Learning by making mistakes
  * Today: 
      - Linear Models
      - Perceptron Alg
      - Perceptron Mistake Bound
      - Online vs Batch Learning
      - Variants of Perceptron
      - Moral for life (lol)

<h5>Recap: X as a vector space</h5>

  * X is an n-dim vector space (ie R^n)
  * each x in X is a feature vector
  * think of x = [x1, x2, ..., xN]

<h5>Recap: Linear Classifiers</h5>

  * Input is an n dim vector x
  * output is a label y in {-1, 1}
  * Linear Threshold Units classify an ex x using the rule
      - output = sgn(w_T * x + b) = sgn(b + sum(w_i*x_i))
          + w_T * x + b >= 0: predict y=1
          + w_T * x + b < 0: predict y=-1
          + b is the bias term
  * Hypothesis space: linear model
      - set of lines that separate the data
      - w_T * x + b = 0 forms a line, where w_T and x are parameters to rep a linear function
          + We cair only about the sign, not the magnitude. if w_T * x + b > 0, one classification, if <0, another classification
      - In dimensions, a linear clasifier represents a hyper-plane that separates the space into two half-spaces
  * Simple trick to remove bias term b
      - w_T * x + b = [wT b] dot [x 1] 
      - = ~w dot ~x, ~w = [w_T * b]_T_ , ~x  = [x_T * 1]_T_
          + basically adding a dimension to the vector and storing the x in that spot


<h5>How to find the best linear model?</h5>

  * Linear classifiers
      - sev algs and models
      - Perceptron
      - Linear suppor vector machines
      - logistic regression
      - ...
      - Based on your assumptions, you get different linear models

<h5>The Perceptron</h5>

  * Perceptron alg
      - Rosenblatt 1958
      - Goal is to find a separating hyperplane
          + for separable data, guaranteed to find one
      - Online algorithm
          + processes one ex at a time
      - Converges if data is separable
          + mistake bound
      - Several variants exist

```python
    # Given a training set D = {(x,y)} 
    Initialize w = 0 in R^N
    for (x,y) in D:
        if y*(w_T * x) <= 0: # Assuming y in {-1, 1}. Made a mistake and need to 
            w = w + yx     # update
    return w 
```
Prediction: y_test = sgn(w_T *  x_test)

<h5>Intuition behind the update</h5>

  * Suppose we have made a mistake on a positive example
  * That is, y = +1 and w_T * x <= 0
  * (**1**)

<h5>Perceptron Learnability</h5>

  * Cannot learn what it cannot rep
      - only linearly separable functions
  * Minsky and Papert demonstrating perceptron's representational limitiations
      - Parity functs can't be learned (XOR)
          + but we already know that XOR is not linearly separable

---

<h5>Convergence</h5>

  * Convergence Theorem:
      - if there exists a set of weights that are consistent with the data (ie the data is linearly separable) the perceptron alg will converge
  * Cycling Theorem
      - If the training data is NOT linearly separable, the learning algorithm will eventually repeat the ame set of weights and enter an infinite loop

<h5>Linear Separability</h5>

  * There exists a hyperplane that can separate the data (we might not know where it is)
  * Margin: distance between the hyperplane and the data pt nearest to it
  * margin of data set (gamma) is the max margin possible for the dataset using any weight data

<h5>Mistake Bound Theorem</h5>

  * Let {(x1, y1), (x2, y2), ..., (xm, ym)} be a sequence of trainingexamples such that for all i the feature vector x_i in R^n hsa magnitude ||x_i|| <= R and the label y_i is in {-1, 1}. 
      - simplified: suppose we have a binary classification dataset with n dimensional inputs
  * Suppose there exists a unit vector u in R^n s/t for some gamma>0 we have Y_i * (u_T * x_i) >= gamma
      - simplified: if the data is separable
  * Then, the perceptron alg will take at most (R/gamma)^2 mistakes on the training sequence 
      - simplified: then the perceptron algorithm will find a separating hyperplane after making a finite number of mistakes


<h5>Intuition</h5>

  * After update, u_T * w_t+1 gets longer than u_T * w_t
  * The size of ||w_t+1|| may increase, but not by very much
  * peep slides for thorough proofs

---
  
<h5>Beyond the separable case</h5>

  * Good News
      - makes no assumption about data, could even be adversarial
      - after a fixed # of mistakes, you're done
  * Bad News
      - real world is NOT linearly separable
          + can't expect to NEVER make mistakes again
          + what we can do: 
              * more features, try to be linearly separable if you can, use averaging
  * 